{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8271978,"sourceType":"datasetVersion","datasetId":4911413}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T05:43:03.529732Z","iopub.execute_input":"2024-05-15T05:43:03.530471Z","iopub.status.idle":"2024-05-15T05:43:07.800929Z","shell.execute_reply.started":"2024-05-15T05:43:03.530437Z","shell.execute_reply":"2024-05-15T05:43:07.800051Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"SOW_token = 0\nEOW_token = 1\nx_train = pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_train.csv', header=None) #, nrows=10000)\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.letter2index = {}\n        self.letter2count = {}\n        self.index2letter = {0: \"0\", 1: \"1\"}\n        self.n_letters = 2 # Count SOW and EOW\n\n    def addWord(self, word):\n        for ch in word:\n            self.addLetter(ch)\n\n    def addLetter(self, ch):\n        if ch not in self.letter2index:\n            self.letter2index[ch] = self.n_letters\n            self.letter2count[ch] = 1\n            self.index2letter[self.n_letters] = ch\n            self.n_letters += 1\n        else:\n            self.letter2count[ch] += 1","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:07.802770Z","iopub.execute_input":"2024-05-15T05:43:07.803291Z","iopub.status.idle":"2024-05-15T05:43:07.944301Z","shell.execute_reply.started":"2024-05-15T05:43:07.803265Z","shell.execute_reply":"2024-05-15T05:43:07.943239Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"input_lang = Lang('eng')\noutput_lang = Lang('hin')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:07.945798Z","iopub.execute_input":"2024-05-15T05:43:07.946427Z","iopub.status.idle":"2024-05-15T05:43:07.952735Z","shell.execute_reply.started":"2024-05-15T05:43:07.946384Z","shell.execute_reply":"2024-05-15T05:43:07.951545Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 45\n\ndef indexesFromWord(lang, word):\n    return [lang.letter2index[ch] for ch in word]\n\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef wordFromTensor(lang, tensor):\n    s = \"\"\n    for i in tensor:\n        if(i.item()==1):\n            break\n        s += lang.index2letter[i.item()] \n    return s\n\ndef get_dataloader(x, input_lang, output_lang, batch_size):\n    n = len(x[0])\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for i in range(n):\n        input_lang.addWord(x[0][i])\n        output_lang.addWord(x[1][i])\n        inp_ids = indexesFromWord(input_lang, x[0][i])\n        tgt_ids = indexesFromWord(output_lang, x[1][i])\n        inp_ids.append(EOW_token)\n        tgt_ids.append(EOW_token)\n        input_ids[i, :len(inp_ids)] = inp_ids\n        target_ids[i, :len(tgt_ids)] = tgt_ids\n    \n    data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                               torch.LongTensor(target_ids).to(device))\n\n    sampler = RandomSampler(data)\n    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:07.955775Z","iopub.execute_input":"2024-05-15T05:43:07.956180Z","iopub.status.idle":"2024-05-15T05:43:07.973660Z","shell.execute_reply.started":"2024-05-15T05:43:07.956142Z","shell.execute_reply":"2024-05-15T05:43:07.972520Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size #32\n        self.embedding = nn.Embedding(input_size, hidden_size) #45x32 = 1440\n        # model\n        self.algo = nn.GRU(hidden_size, hidden_size, num_layers=5, batch_first=True) \n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, input):\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.algo(embedded)\n        return output, hidden","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:07.974735Z","iopub.execute_input":"2024-05-15T05:43:07.974981Z","iopub.status.idle":"2024-05-15T05:43:07.986346Z","shell.execute_reply.started":"2024-05-15T05:43:07.974960Z","shell.execute_reply":"2024-05-15T05:43:07.985553Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.algo = nn.GRU(hidden_size, hidden_size, num_layers=5, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOW_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n            decoder_outputs.append(decoder_output)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n\n    def forward_step(self, input, hidden):\n        output = self.embedding(input)\n        output = F.relu(output)\n        output, hidden = self.algo(output, hidden)\n        output = self.out(output)\n        return output, hidden","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:07.987515Z","iopub.execute_input":"2024-05-15T05:43:07.987855Z","iopub.status.idle":"2024-05-15T05:43:07.999150Z","shell.execute_reply.started":"2024-05-15T05:43:07.987831Z","shell.execute_reply":"2024-05-15T05:43:07.998084Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score \n\ndef train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n          decoder_optimizer, criterion, batch_size):\n\n    total_loss = 0\n    correct = 0\n    all_preds=[]\n    all_labels=[]\n    k = 0\n    \n    for data in dataloader:\n        input_tensor, target_tensor = data\n        \n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n      \n        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n        \n        outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))\n        labels = target_tensor.view(-1)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        \n        if(k%100==0):\n            print(k*32, loss.item(), correct)\n            print(wordFromTensor(input_lang, input_tensor[0]), wordFromTensor(output_lang, target_tensor[0]), wordFromTensor(output_lang, predicted[:45]))\n        k += 1\n#         all_preds.append(predicted.tolist())\n#         all_labels.append(labels.tolist())\n        \n#         i += 1\n#         if(i==1):\n#             print(i, encoder_outputs.shape, encoder_hidden.shape)\n#             print(\" \", outputs.shape, labels.shape, predicted.shape)\n#             print(\" \", predicted[:45], labels[:45])\n        i = 0\n        while (i < 1440):\n            j = 0\n#             mi = 45*i\n            while (j<45):\n                if(predicted[i+j] != labels[i+j]):\n                    break\n                j+=1\n            if(j==45):\n                correct += 1\n            i += 45\n    \n#     print(all_preds)\n#     print(all_labels)\n    print('\\n')\n    return total_loss / len(dataloader), correct ","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:08.000512Z","iopub.execute_input":"2024-05-15T05:43:08.000845Z","iopub.status.idle":"2024-05-15T05:43:08.546361Z","shell.execute_reply.started":"2024-05-15T05:43:08.000808Z","shell.execute_reply":"2024-05-15T05:43:08.545269Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, batch_size=50):\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(1, n_epochs + 1):\n        loss, acc = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size)\n        print(epoch, loss, acc)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:08.547755Z","iopub.execute_input":"2024-05-15T05:43:08.548135Z","iopub.status.idle":"2024-05-15T05:43:08.555598Z","shell.execute_reply.started":"2024-05-15T05:43:08.548094Z","shell.execute_reply":"2024-05-15T05:43:08.554535Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def evaluate(encoder, decoder, word, input_lang, output_lang):\n    with torch.no_grad():\n        input_tensor = tensorFromWord(input_lang, word)\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n\n        _, topi = decoder_outputs.topk(1)\n        decoded_ids = topi.squeeze(-1)\n\n        decoded_letters = []\n        for idx in decoded_ids:\n            if idx.item() == EOW_token:\n                decoded_letters.append['1']\n                break\n            decoded_letters.append(output_lang.index2letter[idx.item()])\n    return decoded_letters\n\ndef evaluateRandomly(encoder, decoder, input_lang, output_lang, n=10):\n    for i in range(n):\n        print('in', x_train[0][i])\n        print('out', x_train[1][i])\n        output_words = evaluate(encoder, decoder, x_train[0][i], input_lang, output_lang)\n        output_sentence = ' '.join(output_words)\n        print('pred', output_sentence)\n        print('')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:08.557217Z","iopub.execute_input":"2024-05-15T05:43:08.558014Z","iopub.status.idle":"2024-05-15T05:43:08.569533Z","shell.execute_reply.started":"2024-05-15T05:43:08.557956Z","shell.execute_reply":"2024-05-15T05:43:08.568435Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"hidden_size = 128\nbatch_size = 32\n\ntrain_dataloader = get_dataloader(x_train, input_lang, output_lang, batch_size)\n# test_dataloader = get_dataloader(x_test, input_lang, output_lang, batch_size)\n\nencoder = EncoderRNN(input_lang.n_letters, hidden_size).to(device)\ndecoder = DecoderRNN(hidden_size, output_lang.n_letters).to(device)\nprint(input_lang.n_letters, output_lang.n_letters)\ntrain(train_dataloader, encoder, decoder, 10, 0.01, batch_size)\n# evaluate\n# encoder.eval()\n# decoder.eval()\n# evaluateRandomly(encoder, decoder, input_lang, output_lang)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T05:43:08.573076Z","iopub.execute_input":"2024-05-15T05:43:08.573526Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"28 66\n0 4.147501468658447 0\naccumulator एक्यूमुलेटर ॅइइइइइइइइइइइइइइइइइइइइइझझझझझझझझझझझझझझझझझझझझझझझ\n3200 0.740950882434845 0\naccommodotion एकोमोडोशन किर्रा\n6400 0.5725782513618469 0\nawarenesh अवेयरनेश पल्ल्ाा\n9600 0.5909155011177063 0\nkebehtar केबेहतर बाडा\n12800 0.5474437475204468 0\nrangarezi रंगरेज़ी मालाा\n16000 0.5747870802879333 0\nsekhanbilag सेखनबिलग सुबाााय\n19200 0.5049561262130737 0\nazmatein अज़मतें अलाााा\n22400 0.43273666501045227 0\nmaghrol माघरोल माडाेड\n25600 0.42726466059684753 0\nnooranganj नूरनगंज निरााार\n28800 0.5175405740737915 0\nmalayaza मलयज माााा0000000000000000000000000000000000000000\n32000 0.42226630449295044 1\npulindo पुलिंदो पाड्डडा\n35200 0.40757158398628235 1\npixelmator पिक्सलमेटर पिरिराााल\n38400 0.4173910319805145 2\ndugai दुगई दाला\n41600 0.5164890289306641 2\nmatadhikyanchi मताधिक्यांची मृ्ठ्च्याचचा\n44800 0.4086759686470032 2\nrajupark राजूपार्क रां्परर\n48000 0.36685308814048767 3\nallotments अलॉटमेंट्स अलममििसस\n\n\n1 0.5067837456986308 4\n0 0.42245355248451233 0\nkarjafedichya कर्जफेडीच्या कर्राारा\n3200 0.39293527603149414 3\nsajili सजीली संावा\n6400 0.3780660927295685 6\nkbadi कबाड़ी काादीी\n","output_type":"stream"}]}]}