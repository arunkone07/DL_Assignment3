{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8271978,"sourceType":"datasetVersion","datasetId":4911413}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport random\nimport wandb\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nimport os\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-17T16:22:31.742300Z","iopub.execute_input":"2024-05-17T16:22:31.742717Z","iopub.status.idle":"2024-05-17T16:22:31.749208Z","shell.execute_reply.started":"2024-05-17T16:22:31.742687Z","shell.execute_reply":"2024-05-17T16:22:31.748267Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"x_train = pd.read_csv('/kaggle/input/aksharantar_sampled/hin/hin_train.csv', header=None) #, nrows=1000)\nx_val = pd.read_csv('/kaggle/input/aksharantar_sampled/hin/hin_valid.csv', header=None)\nx_test = pd.read_csv('/kaggle/input/aksharantar_sampled/hin/hin_test.csv', header=None)\nsz = x_train[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.751116Z","iopub.execute_input":"2024-05-17T16:22:31.751473Z","iopub.status.idle":"2024-05-17T16:22:31.861751Z","shell.execute_reply.started":"2024-05-17T16:22:31.751441Z","shell.execute_reply":"2024-05-17T16:22:31.860743Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"wandb.login()\n# 3dc8367198d0460ba99efb94e713de7e299e685d","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.863374Z","iopub.execute_input":"2024-05-17T16:22:31.864222Z","iopub.status.idle":"2024-05-17T16:22:31.879756Z","shell.execute_reply.started":"2024-05-17T16:22:31.864194Z","shell.execute_reply":"2024-05-17T16:22:31.878647Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes', \n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'inp_embed_size':{\n            'values': [32, 64, 128, 256]\n        },\n        'num_layers': {\n            'values': [1, 3, 5]\n        },\n        'dropout': {\n            'values': [0.2, 0.3, 0.4]\n        },\n        'lr': {\n            'values': [0.01, 0.001, 0.003]\n        },\n        'hidden_size': {\n            'values': [64, 128, 256]\n        },\n        'bidirectional': {\n            'values': ['Yes','No']\n        },\n        'batch_size': {\n            'values': [32, 64, 128]\n        },\n        'cell_type':{\n            'values': ['rnn', 'gru', 'lstm']\n        }\n    }\n}\n\nalgorithms = {\n    'rnn': nn.RNN,\n    'gru': nn.GRU,\n    'lstm': nn.LSTM\n}\n\n# sweep_id = wandb.sweep(sweep=sweep_config, project='DL_Ass3')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.881439Z","iopub.execute_input":"2024-05-17T16:22:31.882485Z","iopub.status.idle":"2024-05-17T16:22:31.888764Z","shell.execute_reply.started":"2024-05-17T16:22:31.882452Z","shell.execute_reply":"2024-05-17T16:22:31.887839Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"SOW_token = 0\nEOW_token = 1\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.n_letters = 2 # Count SOW and EOW\n        self.letter2index = {}\n        self.letter2count = {}\n        self.index2letter = {0: \"0\", 1: \"1\"}\n\n    def addWord(self, word):\n        for ch in word:\n            self.addLetter(ch)\n\n    def addLetter(self, ch):\n        if ch not in self.letter2index:\n            self.letter2index[ch] = self.n_letters\n            self.letter2count[ch] = 1\n            self.index2letter[self.n_letters] = ch\n            self.n_letters += 1\n        else:\n            self.letter2count[ch] += 1","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.890928Z","iopub.execute_input":"2024-05-17T16:22:31.892015Z","iopub.status.idle":"2024-05-17T16:22:31.901287Z","shell.execute_reply.started":"2024-05-17T16:22:31.891960Z","shell.execute_reply":"2024-05-17T16:22:31.900231Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"input_lang = Lang('eng')\noutput_lang = Lang('hin')","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.902703Z","iopub.execute_input":"2024-05-17T16:22:31.902997Z","iopub.status.idle":"2024-05-17T16:22:31.911815Z","shell.execute_reply.started":"2024-05-17T16:22:31.902963Z","shell.execute_reply":"2024-05-17T16:22:31.910761Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 50\n\ndef indexesFromWord(lang, word):\n    return [lang.letter2index[ch] for ch in word]\n\ndef tensorFromWord(lang, word):\n    indexes = indexesFromWord(lang, word)\n    indexes.append(EOW_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef wordFromTensor(lang, tensor):\n    s = \"\"\n    for i in tensor:\n        if(i.item()==1):\n            break\n        s += lang.index2letter[i.item()] \n    return s\n\ndef get_dataloader(x, input_lang, output_lang, batch_size):\n    n = len(x[0])\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for i in range(n):\n        input_lang.addWord(x[0][i])\n        output_lang.addWord(x[1][i])\n        inp_ids = indexesFromWord(input_lang, x[0][i])\n        tgt_ids = indexesFromWord(output_lang, x[1][i])\n        inp_ids.append(EOW_token)\n        tgt_ids.append(EOW_token)\n        input_ids[i, :len(inp_ids)] = inp_ids\n        target_ids[i, :len(tgt_ids)] = tgt_ids\n    \n    data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                               torch.LongTensor(target_ids).to(device))\n\n    sampler = RandomSampler(data)\n    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.912876Z","iopub.execute_input":"2024-05-17T16:22:31.913272Z","iopub.status.idle":"2024-05-17T16:22:31.935807Z","shell.execute_reply.started":"2024-05-17T16:22:31.913236Z","shell.execute_reply":"2024-05-17T16:22:31.934758Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, config, input_size):\n        super(EncoderRNN, self).__init__()\n        \n        self.bidirectional = False\n        if(config.bidirectional == 'Yes'):\n            self.bidirectional = True\n            \n        self.embedding = nn.Embedding(input_size, config.inp_embed_size)\n        self.algo = algorithms[config.cell_type](config.inp_embed_size, config.hidden_size, config.num_layers, bidirectional = self.bidirectional, batch_first=True)\n        self.dropout = nn.Dropout(config.dropout)\n        \n    def forward(self, input):\n        output, hidden = self.algo(self.dropout(self.embedding(input)))\n        return output, hidden","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.937142Z","iopub.execute_input":"2024-05-17T16:22:31.937416Z","iopub.status.idle":"2024-05-17T16:22:31.948848Z","shell.execute_reply.started":"2024-05-17T16:22:31.937393Z","shell.execute_reply":"2024-05-17T16:22:31.948000Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, config, output_size):\n        super(DecoderRNN, self).__init__()\n        \n        self.config = config\n        self.bidirectional = False\n        if(config.bidirectional == 'Yes'): \n            self.bidirectional = True\n           \n        self.embedding = nn.Embedding(output_size, config.hidden_size)\n        self.algo = algorithms[config.cell_type](config.hidden_size, config.hidden_size, config.num_layers, bidirectional = self.bidirectional, batch_first=True)\n        self.out = nn.Linear(config.hidden_size, output_size)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOW_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n            decoder_outputs.append(decoder_output)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n\n    def forward_step(self, input, hidden):\n        output = F.relu(self.embedding(input))\n        output, hidden = self.algo(output, hidden)\n        output = self.out(output)\n        return output, hidden","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.950307Z","iopub.execute_input":"2024-05-17T16:22:31.950565Z","iopub.status.idle":"2024-05-17T16:22:31.962617Z","shell.execute_reply.started":"2024-05-17T16:22:31.950543Z","shell.execute_reply":"2024-05-17T16:22:31.961556Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n          decoder_optimizer, criterion, batch_size, teacher_forcing = True):\n\n    total_loss = 0\n    correct = 0\n    k = 0\n    \n    for data in dataloader:\n        input_tensor, target_tensor = data\n        \n        target_tensor2 = None\n        if (teacher_forcing):\n            target_tensor2 = target_tensor\n            \n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n      \n        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor2)\n        \n        outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))\n        labels = target_tensor.view(-1)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        \n        i = 0\n        while (i < batch_size * MAX_LENGTH):\n            j = 0\n            while (j < MAX_LENGTH):\n                if(predicted[i+j] != labels[i+j]):\n                    break\n                j+=1\n            if(j==MAX_LENGTH):\n                correct += 1\n            i += MAX_LENGTH\n        k += batch_size\n        \n        if(k%6400==0):\n            print(k, loss.item(), correct)\n#             print(wordFromTensor(input_lang, input_tensor[0]), wordFromTensor(output_lang, target_tensor[0]), wordFromTensor(output_lang, predicted[:45]))\n        \n    return total_loss / len(dataloader), correct / k","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.965670Z","iopub.execute_input":"2024-05-17T16:22:31.966055Z","iopub.status.idle":"2024-05-17T16:22:31.978410Z","shell.execute_reply.started":"2024-05-17T16:22:31.966030Z","shell.execute_reply":"2024-05-17T16:22:31.977740Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def train(train_dataloader, val_dataloader, test_dataloader, encoder, decoder, n_epochs, config):\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=config.lr)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=config.lr)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(1, n_epochs + 1):\n        print(epoch)\n        loss, acc = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size)\n        print(\"Train: accuracy:\", acc, \"loss:\", loss)\n        if(acc<0.01 and epoch>=15):\n            break\n        wandb.log({'train_accuracy': acc})\n        wandb.log({'train_loss': loss})\n        val_loss, val_acc = train_epoch(val_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size, teacher_forcing=False)\n        print(\"Validation: accuracy:\", val_acc, \"Loss:\", val_loss, \"\\n\")\n        wandb.log({'val_accuracy': val_acc})\n        wandb.log({'val_loss': val_loss})\n        \n    test_loss, test_acc = train_epoch(test_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size, teacher_forcing=False)\n    print(\"Test: accuracy:\", test_acc, \"Loss:\", test_loss, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.981514Z","iopub.execute_input":"2024-05-17T16:22:31.981785Z","iopub.status.idle":"2024-05-17T16:22:31.992659Z","shell.execute_reply.started":"2024-05-17T16:22:31.981763Z","shell.execute_reply":"2024-05-17T16:22:31.991825Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def test_once(dataloader, encoder, decoder, config):\n\n    total_loss = 0\n    correct = 0\n    k = 0\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=config.lr)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=config.lr)\n    criterion = nn.NLLLoss()\n\n    for data in dataloader:\n        input_tensor, target_tensor = data\n        target_tensor2 = None\n        \n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n      \n        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor2)\n        \n        outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))\n        labels = target_tensor.view(-1)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n        \n        total_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        \n        i = 0\n        while (i < config.batch_size * MAX_LENGTH):\n            j = 0\n            while (j < MAX_LENGTH):\n                if(predicted[i+j] != labels[i+j]):\n                    break\n                j+=1\n            if(j==MAX_LENGTH):\n                correct += 1\n            i += MAX_LENGTH\n        k += config.batch_size\n        \n        if(k%6400==0):\n            print(k, loss.item(), correct)\n#             print(wordFromTensor(input_lang, input_tensor[0]), wordFromTensor(output_lang, target_tensor[0]), wordFromTensor(output_lang, predicted[:45]))\n        \n    return total_loss / len(dataloader), correct / k","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:31.994096Z","iopub.execute_input":"2024-05-17T16:22:31.994695Z","iopub.status.idle":"2024-05-17T16:22:32.006443Z","shell.execute_reply.started":"2024-05-17T16:22:31.994671Z","shell.execute_reply":"2024-05-17T16:22:32.005576Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# num_epochs = 25\n# # project='DL_Ass3'\n# def main():\n#     with wandb.init() as run:\n# #         wandb.run.name = \n#         train_dataloader = get_dataloader(x_train, input_lang, output_lang, wandb.config.batch_size)\n#         val_dataloader = get_dataloader(x_val, input_lang, output_lang, wandb.config.batch_size)\n#         encoder = EncoderRNN(wandb.config, input_lang.n_letters).to(device)\n#         decoder = DecoderRNN(wandb.config, output_lang.n_letters).to(device)\n#         print(input_lang.n_letters, output_lang.n_letters)\n#         train(train_dataloader, val_dataloader, encoder, decoder, num_epochs, wandb.config)\n\n# wandb.agent(sweep_id, function=main, count=30) # calls main function for count number of times.\n# wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:32.007609Z","iopub.execute_input":"2024-05-17T16:22:32.007991Z","iopub.status.idle":"2024-05-17T16:22:32.019542Z","shell.execute_reply.started":"2024-05-17T16:22:32.007955Z","shell.execute_reply":"2024-05-17T16:22:32.018722Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def evaluate(encoder, decoder):\n    for i in range(10):\n        print('>', x_test[0][i])\n        print('=', x_test[1][i])\n        output = ''\n        \n        with torch.no_grad():\n            input_tensor = tensorFromWord(input_lang, x_test[0][i])\n\n            encoder_outputs, encoder_hidden = encoder(input_tensor)\n            decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n\n            _, topi = decoder_outputs.topk(1)\n            decoded_ids = topi.squeeze()\n\n            decoded_word = ''\n            for idx in decoded_ids:\n                if idx.item() == EOW_token:\n                    decoded_word+='1'\n                    break\n                decoded_word += output_lang.index2letter[idx.item()]\n            print('<', decoded_word)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:32.020681Z","iopub.execute_input":"2024-05-17T16:22:32.021039Z","iopub.status.idle":"2024-05-17T16:22:32.041527Z","shell.execute_reply.started":"2024-05-17T16:22:32.021009Z","shell.execute_reply":"2024-05-17T16:22:32.032869Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"num_epochs = 25\n\nbest_config = {\n    'method': 'bayes', \n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'inp_embed_size':{\n            'values': [256]\n        },\n        'num_layers': {\n            'values': [3]\n        },\n        'dropout': {\n            'values': [0.4]\n        },\n        'lr': {\n            'values': [0.001]\n        },\n        'hidden_size': {\n            'values': [256]\n        },\n        'bidirectional': {\n            'values': ['No']\n        },\n        'batch_size': {\n            'values': [64]\n        },\n        'cell_type':{\n            'values': ['lstm']\n        }\n    }\n}\n\nsweep_id = wandb.sweep(sweep=best_config, project='DL_Ass3')\n\ndef test():\n    with wandb.init() as run:\n        train_dataloader = get_dataloader(x_train, input_lang, output_lang, wandb.config.batch_size)\n        val_dataloader = get_dataloader(x_val, input_lang, output_lang, wandb.config.batch_size)\n        test_dataloader = get_dataloader(x_test, input_lang, output_lang, wandb.config.batch_size)\n        encoder = EncoderRNN(wandb.config, input_lang.n_letters).to(device)\n        decoder = DecoderRNN(wandb.config, output_lang.n_letters).to(device)\n        print(input_lang.n_letters, output_lang.n_letters)\n        train(train_dataloader, val_dataloader, test_dataloader, encoder, decoder, num_epochs, wandb.config)\n        encoder.eval()\n        decoder.eval()\n        evaluate(encoder, decoder)\n        \nwandb.agent(sweep_id, function=test, count=1) # calls main function for count number of times.\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:24:55.309746Z","iopub.execute_input":"2024-05-17T16:24:55.310137Z","iopub.status.idle":"2024-05-17T17:05:08.279530Z","shell.execute_reply.started":"2024-05-17T16:24:55.310109Z","shell.execute_reply":"2024-05-17T17:05:08.278679Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Create sweep with ID: mlm9jc14\nSweep URL: https://wandb.ai/arun_cs23m017/DL_Ass3/sweeps/mlm9jc14\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qmksiw7t with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: No\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: lstm\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinp_embed_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240517_162501-qmksiw7t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/arun_cs23m017/DL_Ass3/runs/qmksiw7t' target=\"_blank\">quiet-sweep-1</a></strong> to <a href='https://wandb.ai/arun_cs23m017/DL_Ass3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/arun_cs23m017/DL_Ass3/sweeps/mlm9jc14' target=\"_blank\">https://wandb.ai/arun_cs23m017/DL_Ass3/sweeps/mlm9jc14</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/arun_cs23m017/DL_Ass3' target=\"_blank\">https://wandb.ai/arun_cs23m017/DL_Ass3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/arun_cs23m017/DL_Ass3/sweeps/mlm9jc14' target=\"_blank\">https://wandb.ai/arun_cs23m017/DL_Ass3/sweeps/mlm9jc14</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/arun_cs23m017/DL_Ass3/runs/qmksiw7t' target=\"_blank\">https://wandb.ai/arun_cs23m017/DL_Ass3/runs/qmksiw7t</a>"},"metadata":{}},{"name":"stdout","text":"28 67\n1\n6400 0.5608136653900146 0\n12800 0.5626096725463867 0\n19200 0.4886035621166229 0\n25600 0.5081834197044373 0\n32000 0.4959050416946411 0\n38400 0.5148583650588989 0\n44800 0.4622240364551544 0\n51200 0.4153648316860199 0\nTrain: accuracy: 0.0 loss: 0.5397494491934777\nValidation: accuracy: 0.0 Loss: 0.5128972926177084 \n\n2\n6400 0.4183557629585266 0\n12800 0.3888621926307678 0\n19200 0.3458881676197052 4\n25600 0.3148304522037506 7\n32000 0.2867000997066498 27\n38400 0.2525024712085724 78\n44800 0.20361042022705078 185\n51200 0.2234729379415512 360\nTrain: accuracy: 0.00703125 loss: 0.3329593143798411\nValidation: accuracy: 0.019775390625 Loss: 0.35497543681412935 \n\n3\n6400 0.20546692609786987 247\n12800 0.19802579283714294 625\n19200 0.1599140167236328 1114\n25600 0.1547127366065979 1680\n32000 0.14978952705860138 2282\n38400 0.14765001833438873 2975\n44800 0.14143699407577515 3767\n51200 0.14393343031406403 4568\nTrain: accuracy: 0.08921875 loss: 0.1732616357319057\nValidation: accuracy: 0.09326171875 Loss: 0.28296171105466783 \n\n4\n6400 0.12805508077144623 904\n12800 0.13286878168582916 1880\n19200 0.12452545762062073 2869\n25600 0.11320385336875916 3932\n32000 0.12082090973854065 5009\n38400 0.12263796478509903 6190\n44800 0.12224432080984116 7375\n51200 0.10060389339923859 8570\nTrain: accuracy: 0.1673828125 loss: 0.12490917344577611\nValidation: accuracy: 0.1279296875 Loss: 0.26306721824221313 \n\n5\n6400 0.11383301019668579 1178\n12800 0.10583925247192383 2555\n19200 0.11220020055770874 3946\n25600 0.09881719946861267 5414\n32000 0.11336468905210495 6861\n38400 0.0882507860660553 8331\n44800 0.09850898385047913 9791\n51200 0.09095413982868195 11323\nTrain: accuracy: 0.22115234375 loss: 0.10326987855136395\nValidation: accuracy: 0.151123046875 Loss: 0.2474304458592087 \n\n6\n6400 0.07032923400402069 1479\n12800 0.08470416814088821 3152\n19200 0.09654386341571808 4815\n25600 0.08813086152076721 6567\n32000 0.087189681828022 8320\n38400 0.08394070714712143 10042\n44800 0.08455076813697815 11750\n51200 0.10036789625883102 13451\nTrain: accuracy: 0.26271484375 loss: 0.08956894470378757\nValidation: accuracy: 0.189697265625 Loss: 0.23416733113117516 \n\n7\n6400 0.10497011244297028 1703\n12800 0.08208192884922028 3630\n19200 0.09338262677192688 5506\n25600 0.08208369463682175 7380\n32000 0.08579079806804657 9303\n38400 0.076500803232193 11214\n44800 0.08451972901821136 13122\n51200 0.07751848548650742 15050\nTrain: accuracy: 0.2939453125 loss: 0.0800365139869973\nValidation: accuracy: 0.19970703125 Loss: 0.23007735423743725 \n\n8\n6400 0.07257137447595596 1822\n12800 0.07458025962114334 3989\n19200 0.051379166543483734 6165\n25600 0.058294735848903656 8294\n32000 0.08068050444126129 10366\n38400 0.07071612030267715 12444\n44800 0.07875755429267883 14600\n51200 0.0740438774228096 16803\nTrain: accuracy: 0.32818359375 loss: 0.07243394002784044\nValidation: accuracy: 0.226806640625 Loss: 0.22202894208021462 \n\n9\n6400 0.0720331072807312 2073\n12800 0.06404931098222733 4426\n19200 0.061681557446718216 6757\n25600 0.07372260093688965 9081\n32000 0.07576416432857513 11428\n38400 0.08188829571008682 13712\n44800 0.06907736510038376 15974\n51200 0.07390961050987244 18312\nTrain: accuracy: 0.35765625 loss: 0.06605826343875379\nValidation: accuracy: 0.259521484375 Loss: 0.20816586329601705 \n\n10\n6400 0.059607163071632385 2270\n12800 0.057867344468832016 4833\n19200 0.053378768265247345 7341\n25600 0.05829383432865143 9939\n32000 0.07417795062065125 12484\n38400 0.050626929849386215 15005\n44800 0.06667795777320862 17419\n51200 0.06243419647216797 19923\nTrain: accuracy: 0.38912109375 loss: 0.06030151518993079\nValidation: accuracy: 0.2890625 Loss: 0.19446158921346068 \n\n11\n6400 0.05215781554579735 2472\n12800 0.05346054956316948 5230\n19200 0.04405251890420914 7996\n25600 0.05489372834563255 10689\n32000 0.04236046224832535 13385\n38400 0.050475090742111206 16124\n44800 0.05069155618548393 18769\n51200 0.06626080721616745 21402\nTrain: accuracy: 0.4180078125 loss: 0.05474967362359166\nValidation: accuracy: 0.31884765625 Loss: 0.18396716041024774 \n\n12\n6400 0.033293940126895905 2748\n12800 0.051134273409843445 5737\n19200 0.043597057461738586 8657\n25600 0.04195424169301987 11605\n32000 0.0464998260140419 14492\n38400 0.054625291377305984 17366\n44800 0.05965451896190643 20231\n51200 0.0525440014898777 23047\nTrain: accuracy: 0.45013671875 loss: 0.05028685263358056\nValidation: accuracy: 0.347900390625 Loss: 0.17401752283331007 \n\n13\n6400 0.04280441254377365 2892\n12800 0.04455302283167839 6062\n19200 0.04249906539916992 9209\n25600 0.04177511855959892 12363\n32000 0.04424813389778137 15412\n38400 0.05195784568786621 18430\n44800 0.0487758070230484 21435\n51200 0.04275744408369064 24384\nTrain: accuracy: 0.47625 loss: 0.045969505759421735\nValidation: accuracy: 0.381103515625 Loss: 0.16540032718330622 \n\n14\n6400 0.0459081307053566 2996\n12800 0.04956328496336937 6399\n19200 0.038860321044921875 9788\n25600 0.044705137610435486 13155\n32000 0.032445136457681656 16468\n38400 0.03980689495801926 19758\n44800 0.041379913687705994 22933\n51200 0.04077707603573799 26128\nTrain: accuracy: 0.5103125 loss: 0.041736914850771425\nValidation: accuracy: 0.394775390625 Loss: 0.15910976997110993 \n\n15\n6400 0.03874734044075012 3290\n12800 0.037064407020807266 6911\n19200 0.038442790508270264 10511\n25600 0.043701037764549255 14028\n32000 0.04728925600647926 17529\n38400 0.037519536912441254 21007\n44800 0.0479653924703598 24433\n51200 0.034540265798568726 27725\nTrain: accuracy: 0.54150390625 loss: 0.037806607910897586\nValidation: accuracy: 0.44189453125 Loss: 0.1481977760558948 \n\n16\n6400 0.03360738605260849 3324\n12800 0.03087696246802807 7187\n19200 0.025317257270216942 10989\n25600 0.03070397861301899 14786\n32000 0.03257729485630989 18415\n38400 0.05090225860476494 22062\n44800 0.031227650120854378 25652\n51200 0.038075435906648636 29160\nTrain: accuracy: 0.56953125 loss: 0.03453076131874695\nValidation: accuracy: 0.46142578125 Loss: 0.1386194583028555 \n\n17\n6400 0.023397119715809822 3581\n12800 0.029981547966599464 7573\n19200 0.020871886983513832 11582\n25600 0.027241511270403862 15533\n32000 0.03903454914689064 19411\n38400 0.028793975710868835 23250\n44800 0.030443916097283363 27049\n51200 0.033524058759212494 30759\nTrain: accuracy: 0.60076171875 loss: 0.031224880653899164\nValidation: accuracy: 0.486083984375 Loss: 0.13532155670691282 \n\n18\n6400 0.019589396193623543 3692\n12800 0.03158676624298096 7885\n19200 0.031061317771673203 12189\n25600 0.02825002186000347 16348\n32000 0.02265082858502865 20473\n38400 0.026100212708115578 24525\n44800 0.03338967263698578 28426\n51200 0.029544813558459282 32281\nTrain: accuracy: 0.63048828125 loss: 0.028592219431884586\nValidation: accuracy: 0.51416015625 Loss: 0.12653625383973122 \n\n19\n6400 0.02287863940000534 3767\n12800 0.021827733144164085 8224\n19200 0.021602407097816467 12614\n25600 0.02105114236474037 16959\n32000 0.023271484300494194 21246\n38400 0.02041509933769703 25519\n44800 0.026730773970484734 29702\n51200 0.026611343026161194 33820\nTrain: accuracy: 0.660546875 loss: 0.025698926619952545\nValidation: accuracy: 0.535400390625 Loss: 0.12381191703025252 \n\n20\n6400 0.02830224297940731 4026\n12800 0.03186611831188202 8535\n19200 0.018010064959526062 13126\n25600 0.020653869956731796 17625\n32000 0.020043568685650826 22081\n38400 0.02122179977595806 26480\n44800 0.02866523340344429 30869\n51200 0.024547480046749115 35171\nTrain: accuracy: 0.68693359375 loss: 0.02350800165790133\nValidation: accuracy: 0.560302734375 Loss: 0.1140108072431758 \n\n21\n6400 0.014785273000597954 4072\n12800 0.019405456259846687 8679\n19200 0.020836837589740753 13382\n25600 0.019040774554014206 18121\n32000 0.023549523204565048 22746\n38400 0.027792813256382942 27341\n44800 0.023244567215442657 31832\n51200 0.02132737636566162 36267\nTrain: accuracy: 0.70833984375 loss: 0.021672560286242516\nValidation: accuracy: 0.610107421875 Loss: 0.10361195710720494 \n\n22\n6400 0.023263631388545036 4243\n12800 0.017333514988422394 9083\n19200 0.018192609772086143 13994\n25600 0.01719563826918602 18874\n32000 0.019017957150936127 23656\n38400 0.02004830911755562 28390\n44800 0.021829945966601372 32953\n51200 0.024392835795879364 37513\nTrain: accuracy: 0.73267578125 loss: 0.019774445281364024\nValidation: accuracy: 0.630859375 Loss: 0.09886474028462544 \n\n23\n6400 0.013293777592480183 4322\n12800 0.01698114722967148 9253\n19200 0.012897830456495285 14241\n25600 0.014563310891389847 19200\n32000 0.019962210208177567 24085\n38400 0.018440715968608856 28828\n44800 0.013986471109092236 33538\n51200 0.0162435881793499 38233\nTrain: accuracy: 0.74673828125 loss: 0.01825783113366924\nValidation: accuracy: 0.6435546875 Loss: 0.09502674615941942 \n\n24\n6400 0.015821371227502823 4375\n12800 0.019497502595186234 9441\n19200 0.013955074362456799 14451\n25600 0.011961209587752819 19506\n32000 0.019246160984039307 24532\n38400 0.0154971769079566 29439\n44800 0.02087085321545601 34238\n51200 0.018633488565683365 39018\nTrain: accuracy: 0.7620703125 loss: 0.017089148592203854\nValidation: accuracy: 0.682373046875 Loss: 0.08669229305814952 \n\n25\n6400 0.016981683671474457 4424\n12800 0.013374857604503632 9560\n19200 0.009644131176173687 14702\n25600 0.014040136709809303 19862\n32000 0.013627354055643082 24964\n38400 0.017883673310279846 29994\n44800 0.016290225088596344 34973\n51200 0.020646030083298683 39879\nTrain: accuracy: 0.77888671875 loss: 0.016000300659798086\nValidation: accuracy: 0.67529296875 Loss: 0.08839171874569729 \n\nTest: accuracy: 0.17236328125 Loss: 0.2454875346738845 \n\n> thermax\n= थरमैक्स\n< ीर्ाक1\n> sikhaaega\n= सिखाएगा\n< केखगग1\n> learn\n= लर्न\n< यरड1\n> twitters\n= ट्विटर्स\n< ट्विटर1\n> tirunelveli\n= तिरुनेलवेली\n< िरुनलेवलल1\n> independence\n= इंडिपेंडेंस\n< इंडफेेंसन््1\n> speshiyon\n= स्पेशियों\n< स्पशियों000000000000000000000000000000000000000000\n> shurooh\n= शुरूः\n< ुरउ1\n> kolhapur\n= कोल्हापुर\n< कौलाफर1\n> ajhar\n= अजहर\n< जह1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▁▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.77889</td></tr><tr><td>train_loss</td><td>0.016</td></tr><tr><td>val_accuracy</td><td>0.67529</td></tr><tr><td>val_loss</td><td>0.08839</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">quiet-sweep-1</strong> at: <a href='https://wandb.ai/arun_cs23m017/DL_Ass3/runs/qmksiw7t' target=\"_blank\">https://wandb.ai/arun_cs23m017/DL_Ass3/runs/qmksiw7t</a><br/> View project at: <a href='https://wandb.ai/arun_cs23m017/DL_Ass3' target=\"_blank\">https://wandb.ai/arun_cs23m017/DL_Ass3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240517_162501-qmksiw7t/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# with wandb.init() as run:\n    \n\n#     train_dataloader = get_dataloader(x_train, input_lang, output_lang, wandb.config.batch_size)\n#     test_dataloader = get_dataloader(x_test, input_lang, output_lang, wandb.config.batch_size)\n#     encoder = EncoderRNN(wandb.config, input_lang.n_letters).to(device)\n#     decoder = DecoderRNN(wandb.config, output_lang.n_letters).to(device)\n#     print(input_lang.n_letters, output_lang.n_letters)\n#     train(train_dataloader, test_dataloader, encoder, decoder, num_epochs, wandb.config)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:58.859298Z","iopub.execute_input":"2024-05-17T16:22:58.859972Z","iopub.status.idle":"2024-05-17T16:22:58.864954Z","shell.execute_reply.started":"2024-05-17T16:22:58.859907Z","shell.execute_reply":"2024-05-17T16:22:58.863845Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# import argparse\n# import wandb\n# import torch\n# import torch.nn as nn\n# from torch import optim\n# import torch.nn.functional as F\n# import numpy as np\n# from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# wandb.login()\n# # 3dc8367198d0460ba99efb94e713de7e299e685d\n\n# algorithms = {\n#     'rnn': nn.RNN,\n#     'gru': nn.GRU,\n#     'lstm': nn.LSTM\n# }\n\n# SOW_token = 0\n# EOW_token = 1\n\n# class Lang:\n#     def __init__(self, name):\n#         self.name = name\n#         self.letter2index = {}\n#         self.letter2count = {}\n#         self.index2letter = {0: \"0\", 1: \"1\"}\n#         self.n_letters = 2 # Count SOW and EOW\n\n#     def addWord(self, word):\n#         for ch in word:\n#             self.addLetter(ch)\n\n#     def addLetter(self, ch):\n#         if ch not in self.letter2index:\n#             self.letter2index[ch] = self.n_letters\n#             self.letter2count[ch] = 1\n#             self.index2letter[self.n_letters] = ch\n#             self.n_letters += 1\n#         else:\n#             self.letter2count[ch] += 1\n\n# input_lang = Lang('eng')\n# output_lang = Lang('hin')\n\n\n# x_train = pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_train.csv', header=None) #, nrows=1000)\n# x_val = pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_valid.csv', header=None)\n# x_test = pd.read_csv('/kaggle/input/aksharantar/aksharantar_sampled/hin/hin_test.csv', header=None)\n# sz = x_train[0]\n\n# MAX_LENGTH = 50\n\n# def indexesFromWord(lang, word):\n#     return [lang.letter2index[ch] for ch in word]\n\n# def tensorFromWord(lang, word):\n#     indexes = indexesFromWord(lang, word)\n#     indexes.append(EOW_token)\n#     return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\n# def wordFromTensor(lang, tensor):\n#     s = \"\"\n#     for i in tensor:\n#         if(i.item()==1):\n#             break\n#         s += lang.index2letter[i.item()] \n#     return s\n\n# def get_dataloader(x, input_lang, output_lang, batch_size):\n#     n = len(x[0])\n#     input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n#     target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n#     for i in range(n):\n#         input_lang.addWord(x[0][i])\n#         output_lang.addWord(x[1][i])\n#         inp_ids = indexesFromWord(input_lang, x[0][i])\n#         tgt_ids = indexesFromWord(output_lang, x[1][i])\n#         inp_ids.append(EOW_token)\n#         tgt_ids.append(EOW_token)\n#         input_ids[i, :len(inp_ids)] = inp_ids\n#         target_ids[i, :len(tgt_ids)] = tgt_ids\n    \n#     data = TensorDataset(torch.LongTensor(input_ids).to(device),\n#                                torch.LongTensor(target_ids).to(device))\n\n#     sampler = RandomSampler(data)\n#     dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n#     return dataloader\n\n# class EncoderRNN(nn.Module):\n#     def __init__(self, config, input_size):\n#         super(EncoderRNN, self).__init__()\n        \n#         self.bidirectional = False\n#         if(config.bidirectional == 'Yes'):\n#             self.bidirectional = True\n            \n#         self.embedding = nn.Embedding(input_size, config.inp_embed_size)\n#         self.algo = algorithms[config.cell_type](config.inp_embed_size, config.hidden_size, config.num_layers, bidirectional = self.bidirectional, batch_first=True)\n#         self.dropout = nn.Dropout(config.dropout)\n        \n#     def forward(self, input):\n#         embedded = self.dropout(self.embedding(input))\n#         output, hidden = self.algo(embedded)\n#         return output, hidden\n\n# class DecoderRNN(nn.Module):\n#     def __init__(self, config, output_size):\n#         super(DecoderRNN, self).__init__()\n        \n#         self.config = config\n#         self.bidirectional = False\n#         if(config.bidirectional == 'Yes'): \n#             self.bidirectional = True\n            \n#         self.embedding = nn.Embedding(output_size, config.hidden_size)\n#         self.algo = algorithms[config.cell_type](config.hidden_size, config.hidden_size, config.num_layers, bidirectional = self.bidirectional, batch_first=True)\n#         self.out = nn.Linear(config.hidden_size, output_size)\n\n#     def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n#         batch_size = encoder_outputs.size(0)\n#         decoder_input = torch.empty(self.config.batch_size, 1, dtype=torch.long, device=device).fill_(SOW_token)\n#         decoder_hidden = encoder_hidden\n#         decoder_outputs = []\n\n#         for i in range(MAX_LENGTH):\n#             decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n#             decoder_outputs.append(decoder_output)\n\n#             if target_tensor is not None:\n#                 # Teacher forcing: Feed the target as the next input\n#                 decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n#             else:\n#                 # Without teacher forcing: use its own predictions as the next input\n#                 _, topi = decoder_output.topk(1)\n#                 decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n#         decoder_outputs = torch.cat(decoder_outputs, dim=1)\n#         decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n#         return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n\n#     def forward_step(self, input, hidden):\n#         output = F.relu(self.embedding(input))\n#         output, hidden = self.algo(output, hidden)\n#         output = self.out(output)\n#         return output, hidden\n\n# def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n#           decoder_optimizer, criterion, batch_size, teacher_forcing = True):\n\n#     total_loss = 0\n#     correct = 0\n#     k = 0\n    \n#     for data in dataloader:\n#         input_tensor, target_tensor = data\n        \n#         target_tensor2 = None\n#         if (teacher_forcing):\n#             target_tensor2 = target_tensor\n            \n#         encoder_optimizer.zero_grad()\n#         decoder_optimizer.zero_grad()\n\n#         encoder_outputs, encoder_hidden = encoder(input_tensor)\n      \n#         decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor2)\n        \n#         outputs = decoder_outputs.view(-1, decoder_outputs.size(-1))\n#         labels = target_tensor.view(-1)\n        \n#         loss = criterion(outputs, labels)\n#         loss.backward()\n\n#         encoder_optimizer.step()\n#         decoder_optimizer.step()\n        \n#         total_loss += loss.item()\n#         _, predicted = torch.max(outputs, 1)\n        \n#         i = 0\n#         while (i < batch_size * MAX_LENGTH):\n#             j = 0\n#             while (j < MAX_LENGTH):\n#                 if(predicted[i+j] != labels[i+j]):\n#                     break\n#                 j+=1\n#             if(j==MAX_LENGTH):\n#                 correct += 1\n#             i += MAX_LENGTH\n#         k += batch_size\n        \n#         if(k%6400==0):\n#             print(k, loss.item(), correct)\n# #             print(wordFromTensor(input_lang, input_tensor[0]), wordFromTensor(output_lang, target_tensor[0]), wordFromTensor(output_lang, predicted[:45]))\n        \n#     return total_loss / len(dataloader), correct / k\n\n# def train(train_dataloader, val_dataloader, encoder, decoder, n_epochs, config):\n#     encoder_optimizer = optim.Adam(encoder.parameters(), lr=config.lr)\n#     decoder_optimizer = optim.Adam(decoder.parameters(), lr=config.lr)\n#     criterion = nn.NLLLoss()\n\n#     for epoch in range(1, n_epochs + 1):\n#         print(epoch)\n#         loss, acc = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size)\n#         print(\"Train: accuracy:\", acc, \"loss:\", loss)\n#         if(acc<0.01 and epoch>=15):\n#             break\n#         wandb.log({'train_accuracy': acc})\n#         wandb.log({'train_loss': loss})\n#         val_loss, val_acc = train_epoch(val_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, config.batch_size, teacher_forcing=False)\n#         print(\"Validation: accuracy:\", val_acc, \"Loss:\", val_loss, \"\\n\")\n#         wandb.log({'val_accuracy': val_acc})\n#         wandb.log({'val_loss': val_loss})\n\n\n# def parse_arguments():\n#     parser = argparse.ArgumentParser(description='Training Parameters')\n\n#     parser.add_argument('-wp', '--wandb_project', type=str, default='DL_assignment_3_A',\n#                         help='Project name used to track experiments in Weights & Biases dashboard')\n#     parser.add_argument('-bs', '--batch_size', type= int, default=, choices = [32, 64, 128], help='Choice of batch size') \n#     parser.add_argument('-lr', '--lr', type= float, default=, choices = [0.01, 0.001, 0.003], help='Learning rates')\n#     parser.add_argument('-ies', '--inp_embed_size', type= int, default=, choices = [0.01, 0.001, 0.003], help='input embedding size')\n#     parser.add_argument('-hs', '--hidden_size', type= int, default=, choices = [64, 128, 256], help='No of neurons in each hidden layer')\n#     parser.add_argument('-nl', '--enc_layers', type= int, default=, choices = [1, 2, 3], help='No of layers in encoder and decoder')\n#     # parser.add_argument('-dl', '--dec_layers', type= int, default=, choices = [1, 2, 3], help='No of layers in decoder')\n#     parser.add_argument('-bd', '--bidirectional', type= str, default='No', choices = ['Yes', 'No'], help='Bidirectional RNN or not')\n#     parser.add_argument('-ct', '--cell_type', type= str, default=, choices = ['rnn', 'gru', 'lstm'], help='Algorithm / RNN cell type')\n#     parser.add_argument('-d', '--dropout', type= float, default='no', choices = [0.2, 0.3, 0.4], help='Dropout probability')  \n#     return parser.parse_args()\n\n# args = parse_arguments()\n\n# # sweep_id = wandb.sweep(sweep=sweep_config, project='DL_Assignment3')\n# wandb.init(project=args.wandb_project)\n# config = {\n#     'batch_size': args.batch_size,\n#     'lr': args.lr,\n#     'inp_embed_size': args.inp_embed_size,\n#     'hidden_size':args.hidden_size,\n#     'num_layers': args.num_layers,\n#     'bidirectional': args.bidirectional,\n#     'cell_type': args.cell_type,\n#     'dropout': args.dropout\n# }\n\n# num_epochs = 25\n\n# wandb.init()\n# train_dataloader = get_dataloader(x_train, input_lang, output_lang, wandb.config.batch_size)\n# val_dataloader = get_dataloader(x_val, input_lang, output_lang, wandb.config.batch_size)\n# test_dataloader = get_dataloader(x_test, input_lang, output_lang, wandb.config.batch_size)\n# encoder = EncoderRNN(wandb.config, input_lang.n_letters).to(device)\n# decoder = DecoderRNN(wandb.config, output_lang.n_letters).to(device)\n# print(input_lang.n_letters, output_lang.n_letters)\n# train(train_dataloader, val_dataloader, encoder, decoder, num_epochs, wandb.config)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-17T16:22:58.868080Z","iopub.execute_input":"2024-05-17T16:22:58.868577Z","iopub.status.idle":"2024-05-17T16:22:58.885034Z","shell.execute_reply.started":"2024-05-17T16:22:58.868551Z","shell.execute_reply":"2024-05-17T16:22:58.884191Z"},"trusted":true},"execution_count":54,"outputs":[]}]}